inference-service:
  api-key: xxx
  endpoint: http://127.0.0.1:1234/v1 # LLM provider endpoint (OpenAI compatible)
  concurrency-limit: 1 # How many requests can be executed parallely
  timeout: 600 # Seconds we are willing to wait for the endpoint to reply with AI response
batch:
  input:
    type: INSTAGRAM_EXPORT # INSTAGRAM_EXPORT or WHATSAPP_EXPORT
    path: ./input/ # Path to the directory containing the exported chat files
  output:
    type: TXT # TXT, JSON, NDJSON
    path: ./output/ # Path to the directory where the output will be written
    merge-to-one-file: true # Generate only one outout file, instead of a file for every day
    export-chat-log: false # Add the raw chat log to the output file
logs:
  level: INFO
parsing:
  # chars-per-token: How many characters is a token composed of?
  #  The tokenization efficiency varies for each language, but generally follows this hierarchy:
  #   English: ~4 chars/token
  #   Western European Languages: ~2.5 chars/token
  #   Eastern European (Cyrillic): ~1 chars/token
  #   East Asian Languages: ~1 chars/token or less
  # You can get an estimate by slamming a wikipedia article on https://platform.openai.com/tokenizer
  # (or a tokenizer specific for your model) and dividing the character count by the token count.
  chars-per-token: 4.0 # Read above
  chat-sessions:
    enabled: true # End the day at users sleep time instead of midnight
    sleep-window-end-hour: 9 # End of "sleep detection" window. Usually the Earliest hour of the "Goodmorning" text
    sleep-window-start-hour: 2 # Start of "sleep detection" window. Usually the Earliest hour of the "Goodnight" text
  ignore-chat:
    enabled: false # Ignore messages sent before and after these dates
    ignore-before: "1990-01-01"
    ignore-after: "2150-01-01"
  messages:
    user-interactions:
      message-like: "Ha messo \"Mi piace\" a un messaggio"
      message-reaction: "Ha aggiunto la reazione"
      call-start: "avviato una videochiamata"
    user-content:
      call-start: "[Call started]"
      call-end: "[Call ended]"
      posts-and-reels: "[Shared an internet video]"
      video-uploads: "[Sent a video of himself]"
      photo-uploads: "[Sent a photo of himself]"
      audio-messages: "[Sent an audio message]"
summarization:
  strategy: MAP_REDUCE # LINEAR or MAP_REDUCE
  # linear-strategy: Fit a day's worth of messages into LLM context, and pray the attention god.
  # only really useful if the conversations are short, or if you have access to a high-end model without token concerns.
  linear-strategy:
    max-tokens: 2000 # maximum LLM output length
    model-name: gemma-3-4b-it-qat # The LLM model to be used
    temperature: 0.4
    top-p: 0.7
    system-prompt: |- # Explains to the LLM what it's supposed to do
      You are a bot that writes simple diary entries.
      Below are messages from one day of Instagram DMs.
      Each message starts with the sender name, then a colon, then the text.
      Your job is to write a short diary entry that summarizes what the user did or talked about that day, based only on the provided messages.
    user-prompt: |-
      MESSAGES:
      {messages}
      
      Now, following the instructions step-by-step, write the diary page.
  # map-reduce-strategy: Break a day worth of messages into smaller chunks (approximately token-per-chunk tokens long).
  # Each chunk is processed by the map-agent, in order to produce a mini-summary that extract key info.
  # Every mini-summary is then shoved into the reduce-agent, that produces the actual journal entry.
  #
  # Reduces significantly the used context, in exchange for more LLM calls.
  # Having a simpler job, agents should perform better, and the final journal entry should retain more details, given
  # a nice enough prompt.
  # You can also cut costs by using smaller models as agents.
  map-reduce-strategy:
    token-per-chunk: 4000 # How many tokens worth of messages should the agent chunk together to generate a mini-summary
    map-agent:
      max-tokens: 2000 # maximum LLM output length
      model-name: gemma-3-4b-it-qat # The LLM model to be used
      temperature: 0.2
      top-p: 0.7
      system-prompt: |- # Explains to the LLM what it's supposed to do
        You are an assistant who analyzes blocks of chat messages to extract key information.
        Analyze this block of messages and create a structured summary of activities, conversations, and events.
        Each message starts with the sender name, then a colon, then the text.
        SPECIFIC INSTRUCTIONS:
        1. Identify the main events
        2. Note the emotions expressed
        3. Record important conversation topics
        4. Maintain the chronological order of events
        5. Include only facts present in the messages
      user-prompt: |-
        MESSAGES:
        {messages}
  
        Now, following the instructions step-by-step, write the summary.
    reduce-agent:
      max-tokens: 2000 # maximum LLM output length
      model-name: gemma-3-4b-it-qat # The LLM model to be used
      temperature: 0.4
      top-p: 0.7
      system-prompt: |- # Explains to the LLM what it's supposed to do
        You are an assistant who writes personal journal entries by combining message block summaries.
        You will receive several structured message block summaries from the same day.
        Each message starts with the sender name, then a colon, then the text.
        Your job is to write a short diary by combining all summaries into a single coherent journal entry.
        CRITICAL INSTRUCTIONS:
        1. Write in first person
        2. Organize all events chronologically
        3. Create a smooth narrative flow between different blocks
        4. Maintain the personal and reflective tone of a journal
        5. Eliminate duplication between blocks
        6. Length: 150-250 words
        7. Begin with "Dear Diary," and end with a reflection on the day.
      user-prompt: |-
        MESSAGES:
        {messages}

        Now, following the instructions step-by-step, write the diary page.